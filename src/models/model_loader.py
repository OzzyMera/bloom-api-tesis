from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch

def check_system_info():
    """Verificar informaciÃ³n del sistema"""
    print("ğŸ” InformaciÃ³n del sistema:")
    print(f"   Python: {torch.__version__}")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   CUDA disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print()

def load_bert_classifier():
    """Cargar modelo BERT para clasificaciÃ³n"""
    print("ğŸ¤– Cargando DistilBERT (clasificador)...")
    try:
        # DistilBERT es mÃ¡s rÃ¡pido que BERT-base
        classifier = pipeline(
            "text-classification",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            device=0 if torch.cuda.is_available() else -1
        )
        print("âœ… DistilBERT cargado exitosamente!")
        return classifier
    except Exception as e:
        print(f"âŒ Error cargando BERT: {e}")
        return None

def load_t5_generator():
    """Cargar modelo T5 para generaciÃ³n"""
    print("ğŸ¤– Cargando T5-small (generador)...")
    try:
        # T5-small es perfecto para desarrollo
        generator = pipeline(
            "text2text-generation",
            model="t5-small",
            device=0 if torch.cuda.is_available() else -1
        )
        print("âœ… T5-small cargado exitosamente!")
        return generator
    except Exception as e:
        print(f"âŒ Error cargando T5: {e}")
        return None

def test_bert_classification(classifier):
    """Probar BERT con ejemplos"""
    if not classifier:
        return
    
    print("\nğŸ§ª Probando BERT - AnÃ¡lisis de sentimiento:")
    test_texts = [
        "This is a great API!",
        "I hate bugs in my code",
        "Python is awesome for ML"
    ]
    
    for text in test_texts:
        result = classifier(text)
        label = result[0]['label']
        score = result[0]['score']
        print(f"   ğŸ“ '{text}' â†’ {label} ({score:.2f})")

def test_t5_generation(generator):
    """Probar T5 con ejemplos"""
    if not generator:
        return
    
    print("\nğŸ§ª Probando T5 - GeneraciÃ³n de texto:")
    test_prompts = [
        "translate English to Spanish: Hello world",
        "summarize: The weather is beautiful today. The sun is shining and birds are singing.",
        "question: What is machine learning?"
    ]
    
    for prompt in test_prompts:
        result = generator(prompt, max_length=50, num_return_sequences=1)
        output = result[0]['generated_text']
        print(f"   ğŸ“ Input: '{prompt}'")
        print(f"   ğŸ¤– Output: '{output}'")
        print()

def main():
    """FunciÃ³n principal de prueba"""
    print("ğŸš€ Bloom API - DÃ­a 5: Primer contacto con modelos ML")
    print("=" * 60)
    
    # Verificar sistema
    check_system_info()
    
    # Cargar modelos
    print("ğŸ“¦ Cargando modelos...")
    bert_classifier = load_bert_classifier()
    t5_generator = load_t5_generator()
    
    # Probar modelos
    if bert_classifier:
        test_bert_classification(bert_classifier)
    
    if t5_generator:
        test_t5_generation(t5_generator)
    
    print("\nğŸ¯ Resumen del DÃ­a 5:")
    print(f"   âœ… BERT funcionando: {bert_classifier is not None}")
    print(f"   âœ… T5 funcionando: {t5_generator is not None}")
    
    if bert_classifier and t5_generator:
        print("\nğŸ‰ Â¡Ã‰XITO! Ambos modelos funcionando correctamente")
        print("ğŸš€ Listo para DÃ­a 6: Conectar con FastAPI")
    else:
        print("\nâš ï¸  Revisar errores arriba")

if __name__ == "__main__":
    main()